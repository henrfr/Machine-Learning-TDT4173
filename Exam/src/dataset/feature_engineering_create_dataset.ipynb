{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rurality and shopping area \n",
    "## What happens\n",
    "- The script below computes how many points that are inside a radius of `distance_meters` meters of the point.\n",
    "- Computes how many stores that are closer than `distance_meters` meters from another store for all the stores\n",
    "- Computes how many busstops that are closer than `distance_meters` meters from a store for all the stores\n",
    "- Computes how many busstops that are closer than `distance_meters` meters from another busstop for all the busstops (not sure if this is important or not)\n",
    "- Cumputes the distance to the closest busstop for all stores.\n",
    "\n",
    "## Computational improvements\n",
    "### Initial attempt\n",
    "- Pure python\n",
    "- Would have run in at least a couple of days to compute the pairs of distances and counts\n",
    "- Not acceptable\n",
    "\n",
    "### Optimizing with numba first attempt\n",
    "- Using `njit` decorator of `numba`\n",
    "- Computation down to about 10 minutes\n",
    "- Still a bit slow for parameter tuning for the `distance_meters`-variable\n",
    "\n",
    "### Parallelizing with numba\n",
    "- Using `prange` and `parallel=True` from `numba` \n",
    "- Cutting the time to a couple of minutes\n",
    "### Further improvements - Cuda\n",
    "- Possibly much faster with GPU, but currently not prioritized\n",
    "\n",
    "### Why not compute all pariwise distances once and store it\n",
    "Whould have taken $8B*50122^2\\approx 20GB$ where `50122` is the total number of stores, and 8B is the size of a float64. Could have changed to int32 which would have reduced the size to 10GB. Could have made the matrix triangular and thus reduced the size to 5GB, but it would still be to big.\n",
    "This would needed to be done for `stores x stores`, `stores x busstops` and possibly `busstops x busstops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "from numba import prange, njit\n",
    "from math import radians, cos, sin, asin, sqrt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops = pd.read_csv('../../data/busstops_norway.csv')\n",
    "stores_extra = pd.read_csv('../../data/stores_extra.csv')\n",
    "stores_train = pd.read_csv('../../data/stores_train.csv')\n",
    "stores_test = pd.read_csv('../../data/stores_test.csv')\n",
    "grunnkrets = pd.read_csv('../../data/grunnkrets_norway_stripped.csv')\n",
    "\n",
    "busstops['geometry'] = busstops['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(busstops, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets = grunnkrets.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating the distance in Kilometres   \n",
    "@njit\n",
    "def geo_distance(La1, Lo1, La2, Lo2):  \n",
    "       \n",
    "    # The math module contains the function name \"radians\" which is used for converting the degrees value into radians.  \n",
    "    Lo1 = radians(Lo1)  \n",
    "    Lo2 = radians(Lo2)  \n",
    "    La1 = radians(La1)  \n",
    "    La2 = radians(La2)  \n",
    "        \n",
    "    # Using the \"Haversine formula\"  \n",
    "    D_Lo = Lo2 - Lo1  \n",
    "    D_La = La2 - La1  \n",
    "    P = sin(D_La / 2)**2 + cos(La1) * cos(La2) * sin(D_Lo / 2)**2  \n",
    "   \n",
    "    Q = 2 * asin(sqrt(P))  \n",
    "      \n",
    "    # The radius of earth in kilometres.  \n",
    "    R_km = 6371  \n",
    "        \n",
    "    # calculate result in meters\n",
    "    return(Q * R_km)  * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('i4[:](f8[:,:], f8[:,:], i8, b1)', parallel=True)\n",
    "def number_of_points_closer_than_numba(x, y, distance_meters: int = 1000, x_and_y_same_array=False):\n",
    "    z = np.empty(x.shape[0], dtype=np.int32)\n",
    "    num_rows = x.shape[0]\n",
    "    for i in prange(num_rows):\n",
    "        # number of points closer that distance_meters from point\n",
    "        count = np.count_nonzero(np.array([geo_distance(x[i][0], x[i][1], y[j][0], y[j][1]) < distance_meters \n",
    "            for j in prange(y.shape[0])]))\n",
    "        # remove 1 if x and y are equal as distance to self is 0\n",
    "        if x_and_y_same_array:\n",
    "            count -= 1\n",
    "        z[i] = count\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('Tuple((i4[:], f8[:]))(f8[:,:], f8[:,:], b1)', parallel=True)\n",
    "def closest_points_numba(x, y, x_and_y_same_array=True):\n",
    "    closest_points_distance = np.empty(x.shape[0], dtype=np.float64)\n",
    "    closest_points_index = np.empty(x.shape[0], dtype=np.int32)\n",
    "\n",
    "    for i in prange(x.shape[0]):\n",
    "        # closest point that is not itself\n",
    "        distance_points = np.array([geo_distance(x[i][0], x[i][1], y[j][0], y[j][1]) if x_and_y_same_array == False or x[i][0] != y[j][0] and x[i][1] != y[j][1] else np.inf\n",
    "                                    for j in range(y.shape[0])])\n",
    "        closest_point_index = np.argmin(distance_points)\n",
    "        closest_point_distance = np.min(distance_points)\n",
    "        closest_points_distance[i] = closest_point_distance\n",
    "        closest_points_index[i] = closest_point_index\n",
    "\n",
    "    return closest_points_index, closest_points_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_number_of_points_closer_than(df, points_in_df, geo_points, column_name, distance_meters: int = 1000, x_and_y_same_array=False):\n",
    "    close_elements = number_of_points_closer_than_numba(x=points_in_df, y=geo_points, distance_meters=distance_meters, x_and_y_same_array=x_and_y_same_array)\n",
    "    df[column_name] = close_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_closest_points(df, geo_df, points_in_df, geo_points, index_column_name, new_column_name, x_and_y_same_array=False):\n",
    "    closest_elements_index, closest_elements_distance = closest_points_numba(points_in_df, geo_points, x_and_y_same_array=x_and_y_same_array)\n",
    "    df[new_column_name] = closest_elements_distance\n",
    "    # add id of point that was closest to each point in df \n",
    "    print(closest_elements_distance, closest_elements_index)\n",
    "    df[index_column_name] = geo_df.iloc[closest_elements_index].set_index(df.index)[index_column_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a np array of [lat, lon] from shapely GeoPoints\n",
    "geo_busstops = np.array(list([geo.y, geo.x] for geo in gdf.geometry.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_number_of_points_closer_than(gdf, geo_busstops, geo_busstops, \"num_of_buss_stops_closer_that_1000_to_busstop\", x_and_y_same_array=True)\n",
    "gdf.to_csv(\"../../own_data/busstops_norway_with_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all data of stores together, adding index with train name from where the row came from\n",
    "stores = pd.concat({\"train\": stores_train, \"extra\": stores_extra, \"test\": stores_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the lat lons of the stores, to a numpy array so that the numba-function can compute the distances\n",
    "geo_stores = stores[[\"lat\", \"lon\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 43s, sys: 136 ms, total: 14min 43s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# append number of close stores and close busstops\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_stores, \"other_stores_1000\", x_and_y_same_array=True, distance_meters=1000)\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_stores, \"other_stores_100\", x_and_y_same_array=True, distance_meters=100)\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_stores, \"other_stores_50\", x_and_y_same_array=True, distance_meters=50)\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_stores, \"other_stores_250\", x_and_y_same_array=True, distance_meters=250)\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_busstops, \"buss_stops_1000\", x_and_y_same_array=False, distance_meters=1000)\n",
    "append_number_of_points_closer_than(stores, geo_stores, geo_busstops, \"buss_stops_300\", x_and_y_same_array=False, distance_meters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[495.65295876 102.35551582  25.53176743 ... 114.20099103  63.82726397\n",
      " 125.76760068] [48753   140 58056 ... 30708  9534 56169]\n"
     ]
    }
   ],
   "source": [
    "append_closest_points(\n",
    "    df=stores, \n",
    "    geo_df=busstops, \n",
    "    points_in_df=geo_stores, \n",
    "    geo_points=geo_busstops, \n",
    "    index_column_name='busstop_id', \n",
    "    new_column_name=\"distance_closest_busstop\", \n",
    "    x_and_y_same_array=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all stores that has a grunnkrets_id that doesn't exist in the grunnkrets table\n",
    "stores_wo_grunnkrets = stores.merge(grunnkrets[['grunnkrets_id', 'grunnkrets_name']], how=\"left\", on=\"grunnkrets_id\")\n",
    "nan_stores_all = stores_wo_grunnkrets.loc[stores_wo_grunnkrets['grunnkrets_name'].isna()]\n",
    "outer_stores_test = stores_test.merge(grunnkrets[['grunnkrets_id', 'grunnkrets_name']], how=\"left\", on=\"grunnkrets_id\")\n",
    "nan_stores = outer_stores_test.loc[outer_stores_test['grunnkrets_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.33037557e+02 1.13153469e+02 8.71270649e-01 9.92971322e+03\n",
      " 6.35282105e+03 2.77653490e+04 1.59262209e+03 1.51000246e+04\n",
      " 6.76384356e+03 2.38863345e+02 2.93062827e+03 1.56463351e+03\n",
      " 6.54021281e+03 2.73123562e+04 4.19203250e+02 2.04058526e+04\n",
      " 2.35162647e+04 2.46535005e+03 1.41086385e+03 1.13350247e+04\n",
      " 1.76739494e+03 4.45915791e+03 1.26082022e+04 3.61803004e+03\n",
      " 1.68662920e+03 8.14563288e+03 2.93062827e+03] [47589 16979 14155  1596  7099 46512 49150 14062 28820  7907 49856 14595\n",
      " 16959 10586  1375 11272  2199 14393 14714  9886 13520 25574 46335   659\n",
      " 42344 14702 44173]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624097/1070740115.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[new_column_name] = closest_elements_distance\n",
      "/tmp/ipykernel_624097/1070740115.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[index_column_name] = geo_df.iloc[closest_elements_index].set_index(df.index)[index_column_name]\n"
     ]
    }
   ],
   "source": [
    "# copy grunnkrets of stores so that we can impute the grunnkrets_id of the test_stores that has a bad grunnkrets_id\n",
    "stores['grunnkrets_1'] = stores['grunnkrets_id']\n",
    "append_closest_points(\n",
    "    df=nan_stores,\n",
    "    geo_df=stores[~stores.grunnkrets_id.isin(nan_stores_all.grunnkrets_id)],\n",
    "    points_in_df=nan_stores[[\"lat\", \"lon\"]].to_numpy(),\n",
    "    geo_points=geo_stores,\n",
    "    index_column_name='grunnkrets_1',\n",
    "    new_column_name='closest_store_id',\n",
    "    x_and_y_same_array=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624097/1378478374.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nan_stores.grunnkrets_id = nan_stores.grunnkrets_1\n"
     ]
    }
   ],
   "source": [
    "# rename column of the grunnkrets_id of the stores with bad grunnkrets_id\n",
    "nan_stores.grunnkrets_id = nan_stores.grunnkrets_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grunnkrets_id to grunnkrets_id of the closest store, that is a good grunnkrets_id\n",
    "for _, row in nan_stores[['store_id', 'grunnkrets_1']].iterrows():\n",
    "    stores.loc[stores.store_id == row['store_id'], 'grunnkrets_id'] = row.grunnkrets_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.drop(columns=\"grunnkrets_1\")\n",
    "stores.to_csv(\"../../own_data/stores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv(\"../../own_data/stores.csv\")\n",
    "busstops = pd.read_csv(\"../../own_data/busstops_norway_with_count.csv\")\n",
    "age_dist = pd.read_csv(\"../../data/grunnkrets_age_distribution.csv\")\n",
    "households = pd.read_csv(\"../../data/grunnkrets_households_num_persons.csv\")\n",
    "income = pd.read_csv(\"../../data/grunnkrets_income_households.csv\")\n",
    "grunnkrets = pd.read_csv(\"../../data/grunnkrets_norway_stripped.csv\")\n",
    "plaace = pd.read_csv(\"../../data/plaace_hierarchy.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets.drop(columns=['grunnkrets_name', 'geometry'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets = grunnkrets.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 2015 data and year column\n",
    "households = households.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "income = income.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "age_dist = age_dist.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "grunnkrets.drop(columns='year', inplace=True)\n",
    "households.drop(columns='year', inplace=True)\n",
    "income.drop(columns='year', inplace=True)\n",
    "age_dist.drop(columns='year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to grunnkrets_id for merging\n",
    "households.set_index('grunnkrets_id', inplace=True)\n",
    "grunnkrets.set_index('grunnkrets_id', inplace=True)\n",
    "income.set_index('grunnkrets_id', inplace=True)\n",
    "age_dist.set_index('grunnkrets_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops.drop(columns=['geometry', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = stores.rename(columns={\"Unnamed: 0\": \"dataset\", \"Unnamed: 1\": \"range_index\"}).set_index([\"dataset\", \"range_index\"])\n",
    "stores.drop(columns='sales_channel_name', inplace=True)\n",
    "stores['copy_index'] = stores.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged = stores.merge(grunnkrets, left_on='grunnkrets_id', right_index=True) \\\n",
    "                   .merge(households, left_on='grunnkrets_id', right_index=True, how=\"left\") \\\n",
    "                   .merge(income, left_on='grunnkrets_id', right_index=True, how=\"left\") \\\n",
    "                   .merge(age_dist, left_on='grunnkrets_id', right_index=True, how=\"left\") \\\n",
    "                   .merge(plaace, on='plaace_hierarchy_id') \\\n",
    "                   .merge(busstops, on='busstop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged.index = pd.MultiIndex.from_tuples(\n",
    "    all_merged['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_merged.drop(columns=['copy_index', 'year', 'plaace_hierarchy_id', 'sales_channel_name', 'lv1_desc', 'lv2_desc', 'lv3_desc', 'lv4_desc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged.to_csv(\"../../own_data/all_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens here?\n",
    "- The grunnkrets and age distribution data is loaded.\n",
    "- 2016-values are kept if they exist\n",
    "- Total population and age groups 0-14, 15-64, 64-90 is aggregated\n",
    "- Population density is computed\n",
    "\n",
    "## Remarks\n",
    "- There are 1891 NaNs, around 15%, because we miss age data for some grunnkrets'.\n",
    "\n",
    "## Further improvements\n",
    "- Outlier detection: Small areas yield large densities. Small areas and small populations might be set to NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets = pd.read_csv('../../data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets_ages = pd.read_csv('../../data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_household = pd.read_csv(\"../../data/grunnkrets_households_num_persons.csv\")\n",
    "grunnkrets_household = grunnkrets_household.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "grunnkrets_ages = grunnkrets_ages.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "grunnkrets = grunnkrets.sort_values(by=[\"grunnkrets_id\", \"year\"]).drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets_household.drop(columns=\"year\", inplace=True)\n",
    "grunnkrets_household[\"sum_people\"] = grunnkrets_household.drop(columns=\"grunnkrets_id\").sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grunnkrets_ages.drop_duplicates(subset=[\"grunnkrets_id\"], keep='last')\n",
    "df = grunnkrets.merge(df, how='left', on='grunnkrets_id')\n",
    "df[\"grunnkrets_population\"] = df.drop(columns=[\"grunnkrets_id\", \"year_x\", \"year_y\", \"area_km2\", \"geometry\", \"municipality_name\", \"district_name\", \"grunnkrets_name\"]).sum(axis=1)\n",
    "df = df.merge(grunnkrets_household[['grunnkrets_id', \"sum_people\"]], how=\"left\", on=\"grunnkrets_id\")\n",
    "df['grunnkrets_population'] = df.apply(\n",
    "    lambda row: row['sum_people'] if row['grunnkrets_population'] == 0 else row['grunnkrets_population'],\n",
    "    axis=1\n",
    ")\n",
    "df[\"district_population\"] = df.groupby(\"district_name\")[\"grunnkrets_population\"].transform(\"sum\")\n",
    "df[\"municipality_population\"] = df.groupby(\"municipality_name\")[\"grunnkrets_population\"].transform(\"sum\")\n",
    "df[\"district_area\"] = df.groupby(\"district_name\")[\"area_km2\"].transform(\"sum\")\n",
    "df[\"municipality_area\"] = df.groupby(\"municipality_name\")[\"area_km2\"].transform(\"sum\")\n",
    "ages = [0,3,7,13,18,25,31,41,54,65,78,91]\n",
    "# ages = [0,5,12,18,30,45,65,91]\n",
    "ages = [0,15,35,65,91]\n",
    "# ages = list(range(92))\n",
    "for j in range(1, len(ages)):  \n",
    "    age_from = ages[j-1]\n",
    "    age_to = ages[j]\n",
    "    df[f\"grunnkrets_age_{age_from}-{age_to-1}\"] = df[df.columns[df.columns.isin([f\"age_{i}\" for i in range(age_from, age_to)])]].sum(axis=1)\n",
    "    df[f\"grunnkrets_age_{age_from}-{age_to-1}_distribution\"] = df[f\"grunnkrets_age_{age_from}-{age_to-1}\"]/df[\"grunnkrets_population\"]\n",
    "\n",
    "for name in [\"district_name\", \"municipality_name\"]:\n",
    "    for j, age in enumerate(ages):\n",
    "        prefix = name.split(\"_\")[0]\n",
    "        if j == 0:\n",
    "            pass\n",
    "            # columns = [f\"age_{i}\" for i in range(age)]\n",
    "            # columns.append(name)\n",
    "            # df[f\"{prefix}_age_0-{ages[j]-1}\"] = df[df.columns[df.columns.isin(columns)]].groupby(name).transform(\"sum\").sum(axis=1)\n",
    "        else:\n",
    "            columns = [f\"age_{i}\" for i in range(ages[j-1], age)]\n",
    "            columns.append(name)\n",
    "            df[f\"{prefix}_age_{ages[j-1]}-{ages[j]-1}\"] = df[df.columns[df.columns.isin(columns)]].groupby(name).transform(\"sum\").sum(axis=1)\n",
    "            df[f\"{prefix}_age_{ages[j-1]}-{ages[j]-1}_distribution\"] = df[f\"{prefix}_age_{ages[j-1]}-{ages[j]-1}\"]/df[f\"{prefix}_population\"]\n",
    "# There is currently an error with this, so it is currently discarded\n",
    "# for name in [\"grunnkrets\", \"district\", \"municipality\"]:\n",
    "#     print(df.columns.tolist())\n",
    "#     df[f\"{name}_pop_number\"] = np.sum([df[f\"{name}_age_{ages[j-1]}-{ages[j]-1}\"]*(j - len(ages)/2)/(1-len(ages)/2) for j in range(1, len(ages))])/df[f\"{name}_population\"] \n",
    "df[\"grunnkrets_density\"] = df.grunnkrets_population/df.area_km2\n",
    "df[\"district_density\"] = df.district_population/df.district_area\n",
    "df[\"municipality_density\"] = df.municipality_population/df.municipality_area\n",
    "\n",
    "df = df.drop(columns=[f\"age_{i}\" for i in range(91)])\n",
    "df = df.drop(columns=[\"year_y\"])\n",
    "df = df.rename(columns={\"year_x\": \"year\"})\n",
    "# df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../own_data/grunnkrets_norway_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"../../own_data/all_merged.csv\")\n",
    "all_data_large = pd.read_csv(\"../../own_data/grunnkrets_norway_large.csv\")\n",
    "\n",
    "ages = [0,15, 35,65,91]\n",
    "\n",
    "cols = []\n",
    "for name in [\"district_age\", \"municipality_age\"]:\n",
    "    for j in range(1, len(ages)):  \n",
    "        age_from = ages[j-1]\n",
    "        age_to = ages[j]\n",
    "        col_name = f\"{name}_{age_from}-{age_to-1}\"\n",
    "        # cols.append(col_name)\n",
    "        col_name = col_name + \"_distribution\"\n",
    "        cols.append(col_name)\n",
    "\n",
    "        col_mean, col_std = all_data_large[col_name].mean(), all_data_large[col_name].std()\n",
    "        \n",
    "        # all_data_large[all_data_large[col_name]> col_mean + 3*col_std] = all_data_large[all_data_large[col_name]> col_mean + 3*col_std].apply(lambda x: col_mean +np.random.uniform(-1,1)* col_std)\n",
    "\n",
    "# print(all_data_large['district_age_0-2'])\n",
    "cols.extend(['grunnkrets_id', \n",
    "        'grunnkrets_population',\n",
    "        'district_population', \n",
    "        'municipality_population', \n",
    "        'district_area', \n",
    "        'municipality_area', \n",
    "        'municipality_density', \n",
    "        'district_density', \n",
    "])\n",
    "all_data_large = all_data_large[cols]\n",
    "all_data.set_index([\"dataset\", \"range_index\"], inplace=True)\n",
    "all_data['copy_index'] = all_data.index\n",
    "all_data = all_data.merge(all_data_large, how=\"left\", on='grunnkrets_id')\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "all_data.drop(columns=['age_'+str(i) for i in range(91)], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.groupby([\"lv1\", \"district_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv1_population_district_div_count_stores=lambda gdf:  gdf[\"district_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv2\", \"district_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv2_population_district_div_count_stores=lambda gdf:  gdf[\"district_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "\n",
    "all_data = all_data.groupby([\"lv3\", \"district_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv3_population_district_div_count_stores=lambda gdf:  gdf[\"district_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv4\", \"district_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv4_population_district_div_count_stores=lambda gdf:  gdf[\"district_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv1\", \"municipality_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv1_population_municipality_div_count_stores=lambda gdf:  gdf[\"municipality_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv2\", \"municipality_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv2_population_municipality_div_count_stores=lambda gdf:  gdf[\"municipality_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv3\", \"municipality_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv3_population_municipality_div_count_stores=lambda gdf:  gdf[\"municipality_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data = all_data.groupby([\"lv4\", \"municipality_name\"]) \\\n",
    "    .apply(lambda gdf: \n",
    "        gdf.assign(\n",
    "            lv4_population_municipality_div_count_stores=lambda gdf:  gdf[\"municipality_population\"].mean() / gdf[\"store_id\"].count() \n",
    "        )\n",
    "    ).droplevel(0)\n",
    "\n",
    "all_data.index = pd.MultiIndex.from_tuples(\n",
    "    all_data['copy_index'], \n",
    "    names=['dataset', 'range_index'])\n",
    "\n",
    "all_data.drop(columns='copy_index', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"../../own_data/all_with_stores_pop.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
